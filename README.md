# Entry for SparkCognition Contest
Awrds and Recognition: Semi-finalist
# Problem Statement
Every year the Texas Education Agency evaluates each school district and gives an accountability rating (Met Standard, Met Alternative Standard, or Improvement Required) based on a set of qualities. These qualities pertain to three main areas: student achievement, student progress, and closing performance gaps. Student achievement is evaluated based on the percentage of students who scored at a satisfactory level on standardized tests. Student progress is based on growth by subject and student group. Lastly, the closing performance gap criteria is met by improving academic achievement among student groups based on racial, ethnic, socioeconomic, and other factors. We are looking to find if certain features, beyond the ones used to evaluate the ratings, have any correlation to whether school districts meet Texas education standards. 
We chose to consider the three options for accountability ratings as our labels. The ‘Met Standard’ rating correlates to a campus whose performance has satisfied all criteria established by the TEA. ‘Met Alternative Standard’ is given to alternative education campuses who meet the criteria based on alternative education accountability provisions. In order to be considered an alternative education campus, 75% of students must be at-risk and 50% must be enrolled in grades 6 to 12. ‘Improvement Required’ is assigned to school districts who fail to meet the criteria and are given campus-level intervention.
By looking at factors outside the criteria in evaluating the ratings, we aim to discover other features that could indicate when school districts need intervention. The TEA accountability criteria shows when a Texas school district potentially needs intervention, but when looking at features like attendance rate and dropout rate, we could find some trends that persuade school districts to focus on other attributes to heighten student success rate. Although we know that causality is not the same as correlation, we believe that it is important to look at these trends and analyze them to consider any attribute that could benefit a Texas student’s education.

# Solution
Data used

We used data from the Texas Education Agency .
From this data source, we used data from 2013-2017 to train and test our model. We split this into a training set and testing set based on the years, using the data for 2013-2014, 2014-2015, and 2015-2016 as our training data and the data for 2016-2017 as our test data.
We chose the school district’s accountability rating as our class label and limited it to three potential labels: ‘Met Standard’, ‘Met Alternative Standard’, and ‘Improvement Required’. Part of the cleaning process was selecting only the records from the datasets that had one of these 3 class labels because some school districts were not rated or had unknown values.
We picked out useful features from the dataset, such as attendance rate, dropout rate, and SAT and ACT scores. In the dataset, the column names weren’t particularly useful (i.e. the attendance rate column name for 2016-2017 was DA0AT16R). Therefore, part of the cleaning process was changing the titles for these column names to be more recognizable for us in analyzing trends in the data. This also made the results from our trained model more helpful in identifying what features resulted in a given classification.

Approach and assumptions 

For this project, we closely followed the OSEMN method of performing data science. We gathered a collection of relevant data from TEA and did feature engineering to limit the data to the most relevant features. When realizing that we would need a larger quantity of data, we pulled more data from TEA over other years and performed the same featuring engineering to make our data cohesive. We then cleaned, built the model, and tested it using Darwin services.
Since our data had an obvious class imbalance problem, we had to address this issue by upsampling the minority classes. There were significantly more ‘Met Standard’ labels than ‘Met Alternative Standard’ and ‘Improvement Required’. We did the upsampling using the SMOTENC class from the imbalanced-learn package. The SMOTENC uses SMOTE to upsample the minority class, but it can be used with both nominal and continuous features, rather than just continuous features. This required some cleaning of the data by us, such as removing NaN values or empty strings that were in the place of missing numerical values.
In order to come to an appropriate solution, we made some logical assumptions about the data so that the feature engineering was more coherent. Since we aggregated data from multiple years, we assumed that the relationship between our features and labels is independent with time. We also assumed that each entry in our datasets is unique by not including district name in our analysis.
